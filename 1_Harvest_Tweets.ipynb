{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d723b8b0",
   "metadata": {},
   "source": [
    "## Harvesting tweets using the Twitter API\n",
    "Welcome to Ben's and Rienje's Jupyter Notebooks! In these notebooks, we will showcase step-by-step how we arrived at our final maps and visualizations for our project on Dutch political tweet analysis using geolocation and sentiment analysis. In addition, a ArcGIS story map describing our results can be found [here](https://storymaps.arcgis.com/stories/c09bda1912464431b43bd8bc5fc7de40). This notebook shows how we harvested our tweets with the Twitter API, for which we have used the Twython module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4566e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "\n",
    "from twython import Twython\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de775c7b",
   "metadata": {},
   "source": [
    "#### API keys\n",
    "\n",
    "In order to use the Twitter API, we need a (free) developer account at [Twitter](https://developer.twitter.com/). Here, we can generate the API keys needed for harvesting the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f71edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to the twitter API\n",
    "\n",
    "APP_KEY = 'amWUfdu9uy0ppeo6ntlAT4D6V'\n",
    "APP_SECRET = 'lGlGANJ2ExKMALe9WSVifowTKya1gIdVjYqt8EcMwVXoIx2aPn'\n",
    "OAUTH_TOKEN = '123030766-yDftWoWB0vJIxOy6qBbW1rbJBpdf9klo7qqMl5IG'\n",
    "OAUTH_TOKEN_SECRET = 'zwvtNMq4eOWZDUU2IrcNPVixRnd851Ai8J9Mjsp3XPv8r'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fc248",
   "metadata": {},
   "source": [
    "#### Setting the parameters for our harvest\n",
    "In our project, we made use of the REST API, which made it possible to harvest relatively large quantities of tweets. Because there is a 7-day time window and we needed pre-election tweets, all tweets were harvested in the week leading up to election day (March 17th). First, we need to set the following parameters to narrow down our search:\n",
    "* Query words  \n",
    "* Filters  \n",
    "* Date - window  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General queries to catch tweets mentioning parties\n",
    "generalqueries = '#vvd OR vvd OR #pvv OR pvv OR #cda OR cda OR #d66 OR d66 OR #groenlinks OR groenlinks OR #sp OR sp OR #pvda OR pvda OR #christenunie OR christenunie OR pvdd OR Partij+Voor+De+Dieren OR #50plus OR 50plus OR #sgp OR sgp OR #denk OR #fvd OR fvd OR Forum+voor+Democratie OR #ja21 OR ja21 OR #volt OR volt OR #bij1 OR bij1 OR #bbb OR bbb -filter:retweets -filter:replies -filter:quotes'\n",
    "\n",
    "# Don't harvest retweets, replies or quotes\n",
    "filters = ' -filter:retweets -filter:replies -filter:quotes'\n",
    "\n",
    "# Set an area to retrieve tweets from and maximum data threshold (election date) \n",
    "geo = '52.0907374,5.1214201,250km'\n",
    "date_until = '2021-03-17'\n",
    "\n",
    "# Tweet count needed for the harvest-loop\n",
    "tweet_count = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b4f4",
   "metadata": {},
   "source": [
    "#### Harvesting the tweets with a for loop\n",
    "Because only a limited amount of tweets can be harvested per request, we need a for loop that enables us to harvest 1000 tweets per run (with thanks to Arend!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Twython object \n",
    "twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "# Set parameters for for loop\n",
    "tweets = []\n",
    "MAX_ATTEMPTS = 10\n",
    "COUNT_OF_TWEETS_TO_BE_FETCHED = 1000\n",
    "\n",
    "# Harvesting loop\n",
    "for i in range(0,MAX_ATTEMPTS):\n",
    "    print(\"=========================: \"+str(i))\n",
    "\n",
    "    if (0 == i):\n",
    "        search_results_twy = twitter.search(q=generalqueries, lang= 'nl', count=100, until = date_until)\n",
    "    else:\n",
    "        search_results_twy = twitter.search(q=generalqueries, lang= 'nl', include_entities='true', max_id=next_max_id, count=100)\n",
    "\n",
    "            # Parsing out \n",
    "    for tweet in search_results_twy[\"statuses\"]:  \n",
    "        if tweet['place'] != None:\n",
    "            print(\"%d: %d\" %(len(tweets),tweet['id']))\n",
    "        elif tweet['user']['location'] != None and tweet['user']['location'] != 'Nederland':\n",
    "            print(\"%d: %d\" %(len(tweets),tweet['id']))\n",
    "        \n",
    "        tweet_text = [tweet['id'],tweet['created_at'],tweet['user']['screen_name'], tweet['user']['location'], tweet['text'], tweet['user']['followers_count'],tweet['user']['statuses_count'],tweet['retweet_count'], tweet['favorite_count']]\n",
    "        tweets.append(tweet_text)\n",
    "    try:\n",
    "        # Parse the data returned to get max_id to be passed in consequent call.\n",
    "        next_results_url_params = search_results_twy['search_metadata']['next_results']\n",
    "        next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "    except:\n",
    "       break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae4f43",
   "metadata": {},
   "source": [
    "#### Saving tweets to CSV\n",
    "Lastly, we save the tweets to a csv file. Since we had many files to save, we created a function that automated this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that automates saving tweets to csv\n",
    "def make_csv(filename, tweetdata):\n",
    "    # Convert to df and choose what information to store\n",
    "    tweets_2021 = pd.DataFrame(data=tweetdata,\n",
    "                               columns = ['id','created_at','screen_name','location','text','follower_count','statuses_count','retweet_count','favorite_count'])\n",
    "    # Rename\n",
    "    cols = ['id','created_at','screen_name','location','text','follower_count','statuses_count','retweet_count','favorite_count']\n",
    "    # Remove commas, semicolons and new lines to create a clean CSV \n",
    "    tweets_2021[cols] = tweets_2021[cols].replace({',': '', ';': '','\\\\n':' '}, regex=True)\n",
    "    tweets_2021.to_csv(filename, header=True,index=False)\n",
    "\n",
    "# Save tweets\n",
    "make_csv('output/General_tweets_2021_until17_FNL.csv', tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738a499",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "When starting this project, we initially wanted to analyse geo-tagged tweets. However, we are quite happy that we decided to switch to the location provided in the users profiles. We found that the amount of tweets that have a valid user location is much larger than the amount of geo-tagged tweets that can be harvested from Twitter. Although still only 6000 of 50 000 tweets remained for spatial plotting at the end of the analysis, this could still be seen as a relatively 'good' harvest. When the same method is applied to much larger tweet harvests (say, 500 000 or 5 million), the remainnig user locations-tweets could potentially be a very useful dataset. In addition, profile locations are arguably more reliable as an indicator for where the user might *live* than a geotagged tweet. Users can send tweets from any place, making these locations unreliable as places of residences, but their profile location could be a more trustworthy indicator for their place of residence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
